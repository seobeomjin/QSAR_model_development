{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioalerts\n",
    "from bioalerts import LoadMolecules, Alerts, FPCalculator\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "from rdkit.Chem.Draw import IPythonConsole \n",
    "from rdkit.Chem import PandasTools \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor #knn\n",
    "from sklearn.ensemble import RandomForestRegressor #RF\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. calculate significant substructure feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format of the structures file = SMILES\n",
      "All molecules in the input file were processed correctly\n"
     ]
    }
   ],
   "source": [
    "molecules = bioalerts.LoadMolecules.LoadMolecules(\"./tutorial/datasets/5AR.smi\",name_field=None)\n",
    "molecules.ReadMolecules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n",
      "278\n"
     ]
    }
   ],
   "source": [
    "cox_bio = np.genfromtxt('./tutorial/datasets/5AR.bio.txt',skip_header=0) \n",
    "cox_bio.shape\n",
    "arr = np.arange(0,len(cox_bio))\n",
    "mask = np.ones(arr.shape,dtype=bool)\n",
    "mask[molecules.molserr]=0\n",
    "cox_bio = cox_bio[mask]\n",
    "print len(cox_bio)\n",
    "print len(molecules.mols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = bioalerts.LoadMolecules.GetDataSetInfo(name_field=None)\n",
    "dataset_info.extract_substructure_information(radii=[2,3,4,5,6],mols=molecules.mols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alerts_continuous_high_activity = bioalerts.Alerts.CalculatePvaluesContinuous(radii_ext=[2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.416801226"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_bioactivity_threshold(bio,threshold):\n",
    "    \n",
    "    sorted_bio = sorted(bio)\n",
    "    threshold_bio_value = sorted_bio[int(len(sorted_bio)*threshold)]\n",
    "    return threshold_bio_value\n",
    "\n",
    "threshold_= 0.6\n",
    "threshold_bio_value = calculate_bioactivity_threshold(cox_bio,threshold_)\n",
    "threshold_bio_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_substructure = Alerts_continuous_high_activity.get_significant_substructure_with_high_bioactivity(\n",
    "mols = molecules.mols,\n",
    "     substructure_dictionary = dataset_info.substructure_dictionary,\n",
    "     bioactivities = cox_bio,\n",
    "     mols_ids = molecules.mols_ids[:],\n",
    "     threshold_nb_substructures = 5,\n",
    "     threshold_pvalue = 0.05,\n",
    "     threshold_ratio=0.2,\n",
    "     threshold_high_act_nb_substructures=10,\n",
    "     threshold_high_act_ratio = 0.6,                                                                        \n",
    "     threshold_bioactivity=threshold_bio_value)\n",
    "\n",
    "#threshold_high_act_nb_substructure = N of molecules involved significant substructure with high activity  \n",
    "#threshold_high_act_ratio = N of molecules involved significant substructure with high activity / N of molecules involved significant substructure\n",
    "#threshold_bioactivity = Top 40% by total bioactivity\n",
    "\n",
    "#return value is significant_substructure dictionary with high activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_id_set = set()\n",
    "for k,v in significant_substructure.items():\n",
    "    for mol_id in v: \n",
    "        mol_id_set.add(mol_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. insert feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir =  'C:/jupyter_devel/kist-europe/QSAR/AOP_data/'\n",
    "csv = '5-alpha-reductase-maccs-remcols-stdval-dltnan.csv'\n",
    "result_wdir = 'C:/jupyter_devel/kist-europe/QSAR/AOP_data/model_result/'\n",
    "\n",
    "df = pd.read_csv(wdir+csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Standard Value']\n",
    "x = df.drop(['Molecule','Standard Value'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pic50 = 9 - np.log10(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert feature into only train dataset\n",
    "for iter_ in x.index:\n",
    "    for i in range(8): \n",
    "        if iter_ in mol_id_set : \n",
    "            x.loc[iter_,'significant_feature_{}'.format(i)] = 1.0\n",
    "        else :\n",
    "            x.loc[iter_,'significant_feature_{}'.format(i)] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1 \n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y_pic50,test_size = test_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_idx = []\n",
    "for row in x_test.index:\n",
    "    x_test_idx.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1 knn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration num : 0\n",
      "iteration num : 1\n",
      "iteration num : 2\n",
      "iteration num : 3\n",
      "iteration num : 4\n",
      "[0.5816, 0.9582, 0.6899, 0.8618, 0.5794]\n",
      "('mean : ', 0.73418)\n",
      "iteration num : 5\n",
      "iteration num : 6\n",
      "iteration num : 7\n",
      "iteration num : 8\n",
      "iteration num : 9\n",
      "[0.7596, 0.7974, 0.6246, 0.8691, 0.7737]\n",
      "('mean : ', 0.76488)\n",
      "iteration num : 10\n",
      "iteration num : 11\n",
      "iteration num : 12\n",
      "iteration num : 13\n",
      "iteration num : 14\n",
      "[0.8788, 0.7533, 0.7233, 0.7713, 0.5937]\n",
      "('mean : ', 0.7440800000000001)\n",
      "iteration num : 15\n",
      "iteration num : 16\n",
      "iteration num : 17\n",
      "iteration num : 18\n",
      "iteration num : 19\n",
      "[0.7135, 0.7799, 0.7286, 0.7669, 0.6463]\n",
      "('mean : ', 0.72704)\n",
      "iteration num : 20\n",
      "iteration num : 21\n",
      "iteration num : 22\n",
      "iteration num : 23\n",
      "iteration num : 24\n",
      "[0.6745, 0.8123, 0.71, 0.7142, 0.6853]\n",
      "('mean : ', 0.71926)\n",
      "iteration num : 25\n",
      "iteration num : 26\n",
      "iteration num : 27\n",
      "iteration num : 28\n",
      "iteration num : 29\n",
      "[0.7615, 0.7864, 0.6471, 0.6456, 0.8358]\n",
      "('mean : ', 0.7352799999999999)\n",
      "iteration num : 30\n",
      "iteration num : 31\n",
      "iteration num : 32\n",
      "iteration num : 33\n",
      "iteration num : 34\n",
      "[0.6931, 0.534, 0.7831, 0.7975, 0.8825]\n",
      "('mean : ', 0.73804)\n",
      "iteration num : 35\n",
      "iteration num : 36\n",
      "iteration num : 37\n",
      "iteration num : 38\n",
      "iteration num : 39\n",
      "[0.8879, 0.6478, 0.7063, 0.7039, 0.7298]\n",
      "('mean : ', 0.73514)\n",
      "iteration num : 40\n",
      "iteration num : 41\n",
      "iteration num : 42\n",
      "iteration num : 43\n",
      "iteration num : 44\n",
      "[0.6461, 0.7659, 0.7714, 0.6897, 0.9144]\n",
      "('mean : ', 0.7575000000000001)\n",
      "iteration num : 45\n",
      "iteration num : 46\n",
      "iteration num : 47\n",
      "iteration num : 48\n",
      "iteration num : 49\n",
      "[0.6254, 0.6675, 0.6286, 0.9349, 0.9032]\n",
      "('mean : ', 0.7519199999999999)\n",
      "('total fold mean : ', 0.7407)\n",
      "('total cv mean : ', 0.7407)\n"
     ]
    }
   ],
   "source": [
    "#fold setting\n",
    "kf = RepeatedKFold(n_splits = 5,n_repeats=10)\n",
    "kf.get_n_splits(x_train)\n",
    "#kf\n",
    "#>>> KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "#for numbering k folds\n",
    "k=0\n",
    "cv_rmse = []\n",
    "cv_rmse_mean = []\n",
    "tmp = []\n",
    "\n",
    "#DataFrame generate\n",
    "y_pic_pred_result = pd.DataFrame()\n",
    "y_pic_pred_result['idx'] = pd.Series([i for i in range(len(y))])\n",
    "\n",
    "\n",
    "#model generate\n",
    "neigh = KNeighborsRegressor(n_neighbors = 5,metric='jaccard',weights = 'distance')\n",
    "\n",
    "#cross-val training\n",
    "for train_idx, test_idx in kf.split(x_train):\n",
    "    print(\"iteration num : {}\".format(k))\n",
    "    #print(\"train : \", train_idx, \"\\ntest : \", test_idx)\n",
    "    x_cv_train , x_cv_test = x_train.iloc[train_idx], x_train.iloc[test_idx]\n",
    "    y_cv_train , y_cv_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "    \n",
    "    neigh.fit(x_cv_train,y_cv_train)\n",
    "    \n",
    "    y_cv_testidx = test_idx\n",
    "    y_cv_pred = neigh.predict(x_cv_test)\n",
    "    cv_rmse.append(round(np.sqrt(mean_squared_error(y_cv_test,y_cv_pred)),4))\n",
    "    cv_rmse_mean.append(round(np.sqrt(mean_squared_error(y_cv_test,y_cv_pred)),4))\n",
    "    \n",
    "    y_pic_pred_result['y_cv_testidx_{}'.format(k)] = pd.Series()\n",
    "    y_pic_pred_result['y_cv_pred_{}'.format(k)] = pd.Series()\n",
    "    \n",
    "    for j in range(len(y_cv_testidx)):\n",
    "        y_pic_pred_result.loc[y_cv_testidx[j],'y_cv_testidx_{}'.format(k)] = y_cv_testidx[j]\n",
    "        y_pic_pred_result.loc[y_cv_testidx[j],'y_cv_pred_{}'.format(k)] = y_cv_pred[j]\n",
    "        \n",
    "    \n",
    "    \n",
    "    #y_pic_pred_result['y_cv_testidx_{}'.format(i)] = pd.Series(y_cv_testidx)\n",
    "    #y_pic_pred_result['y_cv_pred_{}'.format(i)] = pd.Series(y_cv_pred)\n",
    "    k += 1 \n",
    "    \n",
    "    if k % 5 == 0 : \n",
    "        tmp.append(np.mean(cv_rmse_mean))\n",
    "        print(cv_rmse_mean) # per fold rmse value in one cv \n",
    "        print(\"mean : \",np.mean(cv_rmse_mean)) # RMSE mean value per iteration of cv  \n",
    "        del cv_rmse_mean[:]\n",
    "        \n",
    "\n",
    "print(\"total fold mean : \", round(np.mean(cv_rmse),4)) # mean of 50 cv rmse (1회 iter돌 때의 모든 rmse의 평균 (50회의 평균)) \n",
    "print(\"total cv mean : \",round(np.mean(tmp),4)) # mean of cv's mean of 1 iter (각 cv rmse 평균의 평균) # we need var of this value \n",
    "\n",
    "#model retrain with all train data  \n",
    "neigh.fit(x_train,y_train) # > model train\n",
    "y_pic_test_pred = neigh.predict(x_test) # > model predidction\n",
    "\n",
    "# \n",
    "y_pic_test_pred_df = pd.DataFrame(columns=['y_pic_test_idx','y_pic_test_pred'])\n",
    "for j in range(len(x_test_idx)):\n",
    "        y_pic_test_pred_df.loc[x_test_idx[j],'y_pic_test_idx'] = x_test_idx[j]\n",
    "        y_pic_test_pred_df.loc[x_test_idx[j],'y_pic_test_pred'] = y_pic_test_pred[j]\n",
    "#y_pic_test_pred_df = pd.DataFrame(y_pic_test_pred,columns=['y_pic_test_pred'])\n",
    "y_pic_pred_result = pd.concat([y_pic_pred_result, y_pic_test_pred_df], axis=1, ignore_index=False)\n",
    "\n",
    "#save csv \n",
    "#y_pic_pred_result.to_csv(result_wdir+'5-alpha-reductase_dltnan_y_pic_pred_result+with_significant_feature.csv',index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total fold rmse mean : ', 0.7407)\n",
      "('total cv rmse mean : ', 0.7407)\n",
      "each cv rmse average : \n",
      "[0.73418, 0.76488, 0.7440800000000001, 0.72704, 0.71926, 0.7352799999999999, 0.73804, 0.73514, 0.7575000000000001, 0.7519199999999999] \n",
      "variance of rmse of every folds : 0.0098 \n",
      "('train data size : 250 (0.0%)\\n', 'test data size : 28 (0.0%)')\n"
     ]
    }
   ],
   "source": [
    "#RESULT\n",
    "print(\"total fold rmse mean : \", round(np.mean(cv_rmse),4)) # mean of 50 cv rmse (1회 iter돌 때의 모든 rmse의 평균 (50회의 평균)) \n",
    "print(\"total cv rmse mean : \",round(np.mean(tmp),4)) # mean of cv's mean of 1 iter (각 cv rmse 평균의 평균) # we need var of this value \n",
    "print(\"each cv rmse average : \\n{} \\nvariance of rmse of every folds : {} \".format(tmp,round(np.var(cv_rmse),4)))\n",
    "print(\"train data size : {} ({}%)\\n\".format(len(x_train),round((len(x_train)/len(x)),2)), \"test data size : {} ({}%)\".format(len(x_test),round((len(x_test)/len(x)),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.2 RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration num : 0\n",
      "iteration num : 1\n",
      "iteration num : 2\n",
      "iteration num : 3\n",
      "iteration num : 4\n",
      "[0.6012, 0.7671, 0.7016, 0.6691, 0.6997]\n",
      "('mean : ', 0.68774)\n",
      "iteration num : 5\n",
      "iteration num : 6\n",
      "iteration num : 7\n",
      "iteration num : 8\n",
      "iteration num : 9\n",
      "[0.7866, 0.6576, 0.6153, 0.5898, 0.6186]\n",
      "('mean : ', 0.65358)\n",
      "iteration num : 10\n",
      "iteration num : 11\n",
      "iteration num : 12\n",
      "iteration num : 13\n",
      "iteration num : 14\n",
      "[0.6836, 0.6286, 0.6577, 0.6397, 0.6106]\n",
      "('mean : ', 0.6440400000000001)\n",
      "iteration num : 15\n",
      "iteration num : 16\n",
      "iteration num : 17\n",
      "iteration num : 18\n",
      "iteration num : 19\n",
      "[0.7629, 0.6269, 0.6652, 0.7297, 0.7313]\n",
      "('mean : ', 0.7032)\n",
      "iteration num : 20\n",
      "iteration num : 21\n",
      "iteration num : 22\n",
      "iteration num : 23\n",
      "iteration num : 24\n",
      "[0.7477, 0.6807, 0.6734, 0.6605, 0.7694]\n",
      "('mean : ', 0.70634)\n",
      "iteration num : 25\n",
      "iteration num : 26\n",
      "iteration num : 27\n",
      "iteration num : 28\n",
      "iteration num : 29\n",
      "[0.5529, 0.7337, 0.6762, 0.8005, 0.7487]\n",
      "('mean : ', 0.7024)\n",
      "iteration num : 30\n",
      "iteration num : 31\n",
      "iteration num : 32\n",
      "iteration num : 33\n",
      "iteration num : 34\n",
      "[0.7045, 0.7706, 0.5034, 0.7032, 0.6681]\n",
      "('mean : ', 0.66996)\n",
      "iteration num : 35\n",
      "iteration num : 36\n",
      "iteration num : 37\n",
      "iteration num : 38\n",
      "iteration num : 39\n",
      "[0.7353, 0.6479, 0.5998, 0.6406, 0.6867]\n",
      "('mean : ', 0.6620600000000001)\n",
      "iteration num : 40\n",
      "iteration num : 41\n",
      "iteration num : 42\n",
      "iteration num : 43\n",
      "iteration num : 44\n",
      "[0.6993, 0.6129, 0.5737, 0.6065, 0.8914]\n",
      "('mean : ', 0.67676)\n",
      "iteration num : 45\n",
      "iteration num : 46\n",
      "iteration num : 47\n",
      "iteration num : 48\n",
      "iteration num : 49\n",
      "[0.5923, 0.7435, 0.7295, 0.6546, 0.6593]\n",
      "('mean : ', 0.67584)\n",
      "('total fold mean : ', 0.6782)\n",
      "('total cv mean : ', 0.6782)\n"
     ]
    }
   ],
   "source": [
    "#fold setting\n",
    "kf = RepeatedKFold(n_splits = 5,n_repeats=10)\n",
    "kf.get_n_splits(x_train)\n",
    "#kf\n",
    "#>>> KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "#for numbering k folds\n",
    "k=0\n",
    "cv_rmse = []\n",
    "cv_rmse_mean = []\n",
    "tmp = []\n",
    "\n",
    "#DataFrame generate\n",
    "y_pic_pred_result = pd.DataFrame()\n",
    "y_pic_pred_result['idx'] = pd.Series([i for i in range(len(y))])\n",
    "\n",
    "\n",
    "#model generate\n",
    "RF = RandomForestRegressor(n_estimators= 30,random_state=0)\n",
    "#cross-val training\n",
    "for train_idx, test_idx in kf.split(x_train):\n",
    "    print(\"iteration num : {}\".format(k))\n",
    "    #print(\"train : \", train_idx, \"\\ntest : \", test_idx)\n",
    "    x_cv_train , x_cv_test = x_train.iloc[train_idx], x_train.iloc[test_idx]\n",
    "    y_cv_train , y_cv_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "    \n",
    "    RF.fit(x_cv_train,y_cv_train)\n",
    "    \n",
    "    y_cv_testidx = test_idx\n",
    "    y_cv_pred = RF.predict(x_cv_test)\n",
    "    cv_rmse.append(round(np.sqrt(mean_squared_error(y_cv_test,y_cv_pred)),4))\n",
    "    cv_rmse_mean.append(round(np.sqrt(mean_squared_error(y_cv_test,y_cv_pred)),4))\n",
    "    \n",
    "    y_pic_pred_result['y_cv_testidx_{}'.format(k)] = pd.Series()\n",
    "    y_pic_pred_result['y_cv_pred_{}'.format(k)] = pd.Series()\n",
    "    \n",
    "    for j in range(len(y_cv_testidx)):\n",
    "        y_pic_pred_result.loc[y_cv_testidx[j],'y_cv_testidx_{}'.format(k)] = y_cv_testidx[j]\n",
    "        y_pic_pred_result.loc[y_cv_testidx[j],'y_cv_pred_{}'.format(k)] = y_cv_pred[j]\n",
    "        \n",
    "    \n",
    "    \n",
    "    #y_pic_pred_result['y_cv_testidx_{}'.format(i)] = pd.Series(y_cv_testidx)\n",
    "    #y_pic_pred_result['y_cv_pred_{}'.format(i)] = pd.Series(y_cv_pred)\n",
    "    k += 1 \n",
    "    \n",
    "    if k % 5 == 0 : \n",
    "        tmp.append(np.mean(cv_rmse_mean))\n",
    "        print(cv_rmse_mean) # per fold rmse value in one cv \n",
    "        print(\"mean : \",np.mean(cv_rmse_mean)) # RMSE mean value per iteration of cv  \n",
    "        del cv_rmse_mean[:]\n",
    "        \n",
    "\n",
    "print(\"total fold mean : \", round(np.mean(cv_rmse),4)) # mean of 50 cv rmse (1회 iter돌 때의 모든 rmse의 평균 (50회의 평균)) \n",
    "print(\"total cv mean : \",round(np.mean(tmp),4)) # mean of cv's mean of 1 iter (각 cv rmse 평균의 평균) # we need var of this value \n",
    "\n",
    "#model retrain with all train data  \n",
    "RF.fit(x_train,y_train) # > model train\n",
    "y_pic_test_pred = RF.predict(x_test) # > model predidction\n",
    "\n",
    "# \n",
    "y_pic_test_pred_df = pd.DataFrame(columns=['y_pic_test_idx','y_pic_test_pred'])\n",
    "for j in range(len(x_test_idx)):\n",
    "        y_pic_test_pred_df.loc[x_test_idx[j],'y_pic_test_idx'] = x_test_idx[j]\n",
    "        y_pic_test_pred_df.loc[x_test_idx[j],'y_pic_test_pred'] = y_pic_test_pred[j]\n",
    "#y_pic_test_pred_df = pd.DataFrame(y_pic_test_pred,columns=['y_pic_test_pred'])\n",
    "y_pic_pred_result = pd.concat([y_pic_pred_result, y_pic_test_pred_df], axis=1, ignore_index=False)\n",
    "\n",
    "#save csv \n",
    "#y_pic_pred_result.to_csv(result_wdir+'5-alpha-reductase_dltnan_y_pic_pred_result+with_significant_feature.csv',index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total fold rmse mean : ', 0.6782)\n",
      "('total cv rmse mean : ', 0.6782)\n",
      "each cv rmse average : \n",
      "[0.68774, 0.65358, 0.6440400000000001, 0.7032, 0.70634, 0.7024, 0.66996, 0.6620600000000001, 0.67676, 0.67584] \n",
      "variance of rmse of every folds : 0.0051 \n",
      "('train data size : 250 (0.0%)\\n', 'test data size : 28 (0.0%)')\n"
     ]
    }
   ],
   "source": [
    "#RESULT\n",
    "print(\"total fold rmse mean : \", round(np.mean(cv_rmse),4)) # mean of 50 cv rmse (1회 iter돌 때의 모든 rmse의 평균 (50회의 평균)) \n",
    "print(\"total cv rmse mean : \",round(np.mean(tmp),4)) # mean of cv's mean of 1 iter (각 cv rmse 평균의 평균) # we need var of this value \n",
    "print(\"each cv rmse average : \\n{} \\nvariance of rmse of every folds : {} \".format(tmp,round(np.var(cv_rmse),4)))\n",
    "print(\"train data size : {} ({}%)\\n\".format(len(x_train),round((len(x_train)/len(x)),2)), \"test data size : {} ({}%)\".format(len(x_test),round((len(x_test)/len(x)),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.3 SVR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold setting\n",
    "kf = RepeatedKFold(n_splits = 5,n_repeats=10)\n",
    "kf.get_n_splits(x_train)\n",
    "#kf\n",
    "#>>> KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "#for numbering k folds\n",
    "k=0\n",
    "cv_rmse = []\n",
    "cv_rmse_mean = []\n",
    "tmp = []\n",
    "\n",
    "#DataFrame generate\n",
    "y_pic_pred_result = pd.DataFrame()\n",
    "y_pic_pred_result['idx'] = pd.Series([i for i in range(len(y))])\n",
    "\n",
    "\n",
    "#model generate\n",
    "\n",
    "\n",
    "#cross-val training\n",
    "for train_idx, test_idx in kf.split(x_train):\n",
    "    print(\"iteration num : {}\".format(k))\n",
    "    #print(\"train : \", train_idx, \"\\ntest : \", test_idx)\n",
    "    x_cv_train , x_cv_test = x_train.iloc[train_idx], x_train.iloc[test_idx]\n",
    "    y_cv_train , y_cv_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "    \n",
    "    RF.fit(x_cv_train,y_cv_train)\n",
    "    \n",
    "    y_cv_testidx = test_idx\n",
    "    y_cv_pred = RF.predict(x_cv_test)\n",
    "    cv_rmse.append(round(np.sqrt(mean_squared_error(y_cv_test,y_cv_pred)),4))\n",
    "    cv_rmse_mean.append(round(np.sqrt(mean_squared_error(y_cv_test,y_cv_pred)),4))\n",
    "    \n",
    "    y_pic_pred_result['y_cv_testidx_{}'.format(k)] = pd.Series()\n",
    "    y_pic_pred_result['y_cv_pred_{}'.format(k)] = pd.Series()\n",
    "    \n",
    "    for j in range(len(y_cv_testidx)):\n",
    "        y_pic_pred_result.loc[y_cv_testidx[j],'y_cv_testidx_{}'.format(k)] = y_cv_testidx[j]\n",
    "        y_pic_pred_result.loc[y_cv_testidx[j],'y_cv_pred_{}'.format(k)] = y_cv_pred[j]\n",
    "        \n",
    "    \n",
    "    \n",
    "    #y_pic_pred_result['y_cv_testidx_{}'.format(i)] = pd.Series(y_cv_testidx)\n",
    "    #y_pic_pred_result['y_cv_pred_{}'.format(i)] = pd.Series(y_cv_pred)\n",
    "    k += 1 \n",
    "    \n",
    "    if k % 5 == 0 : \n",
    "        tmp.append(np.mean(cv_rmse_mean))\n",
    "        print(cv_rmse_mean) # per fold rmse value in one cv \n",
    "        print(\"mean : \",np.mean(cv_rmse_mean)) # RMSE mean value per iteration of cv  \n",
    "        del cv_rmse_mean[:]\n",
    "        \n",
    "\n",
    "print(\"total fold mean : \", round(np.mean(cv_rmse),4)) # mean of 50 cv rmse (1회 iter돌 때의 모든 rmse의 평균 (50회의 평균)) \n",
    "print(\"total cv mean : \",round(np.mean(tmp),4)) # mean of cv's mean of 1 iter (각 cv rmse 평균의 평균) # we need var of this value \n",
    "\n",
    "#model retrain with all train data  \n",
    "RF.fit(x_train,y_train) # > model train\n",
    "y_pic_test_pred = RF.predict(x_test) # > model predidction\n",
    "\n",
    "# \n",
    "y_pic_test_pred_df = pd.DataFrame(columns=['y_pic_test_idx','y_pic_test_pred'])\n",
    "for j in range(len(x_test_idx)):\n",
    "        y_pic_test_pred_df.loc[x_test_idx[j],'y_pic_test_idx'] = x_test_idx[j]\n",
    "        y_pic_test_pred_df.loc[x_test_idx[j],'y_pic_test_pred'] = y_pic_test_pred[j]\n",
    "#y_pic_test_pred_df = pd.DataFrame(y_pic_test_pred,columns=['y_pic_test_pred'])\n",
    "y_pic_pred_result = pd.concat([y_pic_pred_result, y_pic_test_pred_df], axis=1, ignore_index=False)\n",
    "\n",
    "#save csv \n",
    "#y_pic_pred_result.to_csv(result_wdir+'5-alpha-reductase_dltnan_y_pic_pred_result+with_significant_feature.csv',index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESULT\n",
    "print(\"total fold rmse mean : \", round(np.mean(cv_rmse),4)) # mean of 50 cv rmse (1회 iter돌 때의 모든 rmse의 평균 (50회의 평균)) \n",
    "print(\"total cv rmse mean : \",round(np.mean(tmp),4)) # mean of cv's mean of 1 iter (각 cv rmse 평균의 평균) # we need var of this value \n",
    "print(\"each cv rmse average : \\n{} \\nvariance of rmse of every folds : {} \".format(tmp,round(np.var(cv_rmse),4)))\n",
    "print(\"train data size : {} ({}%)\\n\".format(len(x_train),round((len(x_train)/len(x)),2)), \"test data size : {} ({}%)\".format(len(x_test),round((len(x_test)/len(x)),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect as _GetMorganFingerprintAsBitVect, GetMorganFingerprint as _GetMorganFingerprint\n",
    "from bioalerts import LoadMolecules, Alerts, FPCalculator \n",
    "from rdkit import Chem \n",
    "\n",
    "# >>> _GetMorganFingerprint #Returns a Morgan fingerprint for a molecule\n",
    "# >>> _GetMorganFingerprintAsBitVect  #Returns a Morgan fingerprint for a molecule as a bit vector\n",
    "\n",
    "molecules = LoadMolecules.LoadMolecules(\"./tutorial/datasets/5AR.smi\",name_field=None)\n",
    "molecules.ReadMolecules() \n",
    "stride = int(len(molecules.mols)*0.9)\n",
    "training = molecules.mols[0:stride]\n",
    "test = molecules.mols[stride:len(molecules.mols)]\n",
    "print (len(molecules.mols), len(test), len(training))\n",
    "\n",
    "radii = [2,3,4,5,6]\n",
    "\n",
    "\n",
    "def extract_substructure_information(radii,mols):\n",
    "    substructure_dictionary = {}\n",
    "    for i,m in enumerate(mols):\n",
    "        info = {}\n",
    "        fp = _GetMorganFingerprint(m,max(radii),bitInfo=info)\n",
    "        for k,v in info.items():\n",
    "            if v[0][1] in radii:\n",
    "                if k in substructure_dictionary.keys():\n",
    "                    substructure_dictionary[k].append(i)\n",
    "                else : \n",
    "                    substructure_dictionary.update({k:[i]})\n",
    "    return substructure_dictionary\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
